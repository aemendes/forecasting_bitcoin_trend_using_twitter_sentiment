{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b_jWiVUqASU",
    "outputId": "39a6734b-a365-46c5-cb95-acc19cb4ef8c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OU91stusaga"
   },
   "source": [
    "# Load dataset & See stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ausFfSMsrwxO",
    "outputId": "b265ec7e-80c4-4bfa-caf1-a38283aee748"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/raw/datasetBalanced.csv\") \\\n",
    "            .drop_duplicates(subset='text', keep=\"last\")[['text','sentiment']] # Remove duplicates & keep columns to use\n",
    "        \n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rezz0P93r6wn",
    "outputId": "9c20e092-0d24-43af-9ca6-b51116143369"
   },
   "outputs": [],
   "source": [
    "# Get the dataset lenght\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-BUdfs_sGaC",
    "outputId": "b6bd16b6-1c8d-4c1a-aca8-10df3b37955a"
   },
   "outputs": [],
   "source": [
    "# Validate if there are different values from negative (0), neutral (1) and positive (2)\n",
    "dataset['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "31lWOJd3sO5m",
    "outputId": "f81fe73a-5ef4-4a24-97fe-75820ad4d5df"
   },
   "outputs": [],
   "source": [
    "# How distributed is the dataset\n",
    "dataset.groupby('sentiment').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkfKbcp0siDP"
   },
   "source": [
    "# Data cleaning\n",
    "\n",
    "Even when the dataset is a little bit biased, we'll keep it this way because the differences are not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibFtXS0Zs2rb"
   },
   "outputs": [],
   "source": [
    "# Fill null values.\n",
    "dataset[\"text\"].fillna(\"No content\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqF3ZYiSteq5"
   },
   "source": [
    "### The next steps about data cleaning will be:\n",
    "\n",
    "* Remove URLs from the tweets\n",
    "* Tokenize text\n",
    "* Remove emails\n",
    "* Remove new lines characters\n",
    "* Remove distracting single quotes\n",
    "* Remove all punctuation signs\n",
    "* Lowercase all text\n",
    "* Detokenize text\n",
    "* Convert list of texts to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZqC3DjvtKvB"
   },
   "outputs": [],
   "source": [
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "EMOJI_RE = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "\n",
    "def depure_data(data):\n",
    "    #Removing URLs with a regular expression\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "\n",
    "     #Remove @ sign\n",
    "    data = re.sub(\"@[A-Za-z0-9]+\",\"\",data)\n",
    "\n",
    "     #Remove http links\n",
    "    data = re.sub(r\"(?:@|http?://|https?://|www)\\S+\", \"\", data)\n",
    "\n",
    "    #Remove Emojis\n",
    "    data = EMOJI_RE.sub('', data)\n",
    "\n",
    "    #Remove hashtag sign but keep the text\n",
    "    data = re.sub(\"#[A-Za-z0-9]+\",\"\",data)\n",
    "\n",
    "    # Remove html tags\n",
    "    data = TAG_RE.sub('', data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqi7X1Xlwfff",
    "outputId": "8bdf047a-61dd-4d02-8afc-bee9a58f819b"
   },
   "outputs": [],
   "source": [
    "list_words = stopwords.words('english')\n",
    "\n",
    "tweets = dataset['text'].values.tolist()\n",
    "tweets_len = len(tweets)\n",
    "\n",
    "remove_stopwords = lambda data: (\" \").join([word for word in data.split() if not word in list_words]) # Remove unused words like will\n",
    "processor = lambda sentence: simple_preprocess(str(sentence), deacc=True) # Lowecase, ponctuation & accents\n",
    "detokenizer = lambda sequence: TreebankWordDetokenizer().detokenize(sequence) # Join sequence of tokens\n",
    "\n",
    "data = list([detokenizer(processor(remove_stopwords(depure_data(tweets[i])))) for i in range(tweets_len)])\n",
    "\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stTVfGAnturY"
   },
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "\n",
    "# Save cleaned data for performance purposes\n",
    "pd.DataFrame(data).to_csv('./data/prepared/0_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7m9U_BZ3Bmu"
   },
   "source": [
    "# Label encoding\n",
    "\n",
    "As the dataset is categorical, we need to convert the sentiment labels from Neutral, Negative and Positive to a float type that our model can understand. To achieve this task, we'll implement the to_categorical method from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wjujPFZ24qq",
    "outputId": "9b4faa04-438a-4288-a66f-9b43161cf84a"
   },
   "outputs": [],
   "source": [
    "labels = tf.keras.utils.to_categorical(dataset['sentiment'], 3, dtype=\"float32\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3L8Ltlg5KHz"
   },
   "source": [
    "# Data sequencing and splitting\n",
    "\n",
    "We'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZ8x-Nzb5Ddk"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi3B4DFG5Q_z"
   },
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLqvRnMn5VRv",
    "outputId": "ca36f6a8-f0d4-44c8-b867-2540cc1bd099"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWhzkkaB5vcE",
    "outputId": "80ff2c95-b0bc-49e8-944f-7e590d0037d9"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, test_size=0.20, random_state=42)\n",
    "\n",
    "print (len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoSjz1ar6cT1"
   },
   "source": [
    "# Model building\n",
    "\n",
    "Alright, in the next cells I'll guide you through the process of building 3 Recurrent Neural Networks. I'll implement sequential models from the Keras API to achieve this task. Essentially, I'll start with a single layer **LSTM** network which is known by achieving good results in NLP tasks when the dataset is relatively small (I could have started with a SimpleRNN which is even simpler, but to be honest it's actually not deployed in production environments because it is too simple - however I'll leave it commented in case you want to know it's built). The next one will be a Bidirectional LSTM model, a more complex one and this particular one is known to achieve great metrics when talking about text classification. To go beyond the classic NLP approach, finally we'll implement a very unusual model: a Convolutional 1D network, known as well by delivering good metrics when talking about NLP. If everything goes ok, we should get the best results with the BidRNN, let's see what happens.\n",
    "\n",
    "Let's get hands on:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3zv75fm63Ej"
   },
   "source": [
    "## CNN layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "9IkAvgJw5y9r",
    "outputId": "b99c9e05-878e-4b3b-fd2f-da5f4ff39f20"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Dense, MaxPool1D, Flatten, Input, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Embedding(max_words, 30))\n",
    "\n",
    "model.add(Conv1D(60, 15, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint1 = ModelCheckpoint(\"./models/0_cnn_tweet_sentiment.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', period=2, save_weights_only=False)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[checkpoint1], batch_size=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7gnmBYmJgJp"
   },
   "source": [
    "# Best model validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29dEsjgqJnV9",
    "outputId": "14aa97cf-e01e-42ca-eb5d-651d9d4ca842"
   },
   "outputs": [],
   "source": [
    "# Load the best model obtained during training\n",
    "model = keras.models.load_model(\"./models/0_cnn_tweet_sentiment.hdf5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print('Model accuracy: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1NifRANJpmD"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "BhrfjDB9JrO4",
    "outputId": "a762db4e-9099-4f93-81fc-5a55c88aad29"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrjcqUFxLX_H"
   },
   "source": [
    "## Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMgFwrTiLjqu"
   },
   "outputs": [],
   "source": [
    "sentiment = ['Negative', 'Neutral','Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4cAyRIiKOt4"
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this experience has been the best, want my money back'])\n",
    "\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8IMa258Q31T"
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences([' always good very good extremly'])\n",
    "\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4pB34hsLaef"
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are really fantastic'])\n",
    "\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvdpp7TELcxU"
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\n",
    "\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "\n",
    "sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_CHALLENGE_3_MEIA.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a46199d73dd44133214783e09ecb9011b7576ec30647c37c42b990029f742d69"
  },
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
