{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_CHALLENGE_3_MEIA_03.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b_jWiVUqASU",
        "outputId": "407f6053-d1bc-41f6-d2f2-a0f3d12692d4"
      },
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "import os.path\n",
        "from os import path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OU91stusaga"
      },
      "source": [
        "# Load dataset & See stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EomuP7GsruJZ",
        "outputId": "2e157118-3379-48bc-b1fc-4dfda14df5eb"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "ausFfSMsrwxO",
        "outputId": "53116c24-907f-4496-9db6-6949afb37051"
      },
      "source": [
        "dataset = pd.read_csv(\"/content/gdrive/My Drive/Datasets/datasetBalanced.csv\") \\\n",
        "            .drop_duplicates(subset='text', keep=\"last\")[['text','sentiment']] # Remove duplicates & keep columns to use\n",
        "        \n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our palace team will put our hearts out for ou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Chelsea v Palace kick-off time changed and TV ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Academy redevelopment update: Exclusive footag...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@ChelseaFC I still have no entire clue why us ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pizza fight suggestions wrong, #CPFC wonderkid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  Our palace team will put our hearts out for ou...          1\n",
              "1  Chelsea v Palace kick-off time changed and TV ...          1\n",
              "2  Academy redevelopment update: Exclusive footag...          1\n",
              "3  @ChelseaFC I still have no entire clue why us ...          1\n",
              "4  Pizza fight suggestions wrong, #CPFC wonderkid...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rezz0P93r6wn",
        "outputId": "cca6d142-1aff-4aab-aa95-d25dcea3cd22"
      },
      "source": [
        "# Get the dataset lenght\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "860452"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-BUdfs_sGaC",
        "outputId": "1d884344-13ea-4722-f1cc-181f356f72b8"
      },
      "source": [
        "# Validate if there are different values from negative (0), neutral (1) and positive (2)\n",
        "dataset['sentiment'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "31lWOJd3sO5m",
        "outputId": "5feb7f1b-cb6e-4475-aad4-591b5703ebb8"
      },
      "source": [
        "# How distributed is the dataset\n",
        "dataset.groupby('sentiment').nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>266239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>309056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>285157</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             text\n",
              "sentiment        \n",
              "0          266239\n",
              "1          309056\n",
              "2          285157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkfKbcp0siDP"
      },
      "source": [
        "# Data cleaning\n",
        "\n",
        "Even when the dataset is a little bit biased, we'll keep it this way because the differences are not significant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8bBmm9ysqTk"
      },
      "source": [
        "# Fill null values.\n",
        "dataset[\"text\"].fillna(\"No content\", inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqF3ZYiSteq5"
      },
      "source": [
        "### The next steps about data cleaning will be:\n",
        "\n",
        "* Remove URLs from the tweets\n",
        "* Tokenize text\n",
        "* Remove emails\n",
        "* Remove new lines characters\n",
        "* Remove distracting single quotes\n",
        "* Remove all punctuation signs\n",
        "* Lowercase all text\n",
        "* Detokenize text\n",
        "* Convert list of texts to Numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stTVfGAnturY"
      },
      "source": [
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "EMOJI_RE = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "\n",
        "def depure_data(data):\n",
        "    #Removing URLs with a regular expression\n",
        "    data = url_pattern.sub(r'', data)\n",
        "\n",
        "    # Remove Emails\n",
        "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
        "\n",
        "    # Remove new line characters\n",
        "    data = re.sub('\\s+', ' ', data)\n",
        "\n",
        "    # Remove distracting single quotes\n",
        "    data = re.sub(\"\\'\", \"\", data)\n",
        "\n",
        "     #Remove @ sign\n",
        "    data = re.sub(\"@[A-Za-z0-9]+\",\"\",data)\n",
        "\n",
        "     #Remove http links\n",
        "    data = re.sub(r\"(?:@|http?://|https?://|www)\\S+\", \"\", data)\n",
        "\n",
        "    #Remove Emojis\n",
        "    data = EMOJI_RE.sub('', data)\n",
        "\n",
        "    #Remove hashtag sign but keep the text\n",
        "    data = re.sub(\"#[A-Za-z0-9]+\",\"\",data)\n",
        "\n",
        "    # Remove html tags\n",
        "    data = TAG_RE.sub('', data);\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4f89yk91mah",
        "outputId": "f3e9308c-29a0-4375-95de-205880fc0d6a"
      },
      "source": [
        "list_words = stopwords.words('english')\n",
        "\n",
        "tweets = dataset['text'].values.tolist()\n",
        "tweets_len = len(tweets)\n",
        "\n",
        "remove_stopwords = lambda data: (\" \").join([word for word in data.split() if not word in list_words]) # Remove unused words like will\n",
        "processor = lambda sentence: simple_preprocess(str(sentence), deacc=True) # Lowecase, ponctuation & accents\n",
        "detokenizer = lambda sequence: TreebankWordDetokenizer().detokenize(sequence) # Join sequence of tokens\n",
        "\n",
        "data = list([detokenizer(processor(remove_stopwords(depure_data(tweets[i])))) for i in range(tweets_len)])\n",
        "\n",
        "print(data[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['our palace team put hearts fans don billions fans solid family fans', 'chelsea palace kick off time changed tv information confirmed', 'academy redevelopment update exclusive footage shows new site progression', 'still entire clue us crystal palace wouldn go abraham he top shout tbh he look striker maybe next transfer', 'pizza fight suggestions wrong wonderkid says interview national media']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1yMP0um2eMf"
      },
      "source": [
        "data = np.array(data)\n",
        "\n",
        "# Save cleaned data for performance purposes\n",
        "pd.DataFrame(data).to_csv('/content/gdrive/My Drive/Datasets/cleaned_datasetBalanced.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7m9U_BZ3Bmu"
      },
      "source": [
        "# Label encoding\n",
        "\n",
        "As the dataset is categorical, we need to convert the sentiment labels from Neutral, Negative and Positive to a float type that our model can understand. To achieve this task, we'll implement the to_categorical method from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wjujPFZ24qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c954e2c-79e4-4683-e768-3b3e9219219e"
      },
      "source": [
        "labels = tf.keras.utils.to_categorical(dataset['sentiment'], 3, dtype=\"float32\")\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " ...\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3L8Ltlg5KHz"
      },
      "source": [
        "# Data sequencing and splitting\n",
        "\n",
        "We'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ8x-Nzb5Ddk"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi3B4DFG5Q_z"
      },
      "source": [
        "max_words = 5000\n",
        "max_len = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLqvRnMn5VRv",
        "outputId": "9a9dc09d-e38d-47d6-8851-69662e57f07b"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "tweets = pad_sequences(sequences, maxlen=max_len)\n",
        "print(tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ... 1145  434   27]\n",
            " [   0    0    0 ...  723 1782  436]\n",
            " [   0    0    0 ...  894    9 1170]\n",
            " ...\n",
            " [   0    0    0 ...  193  286  150]\n",
            " [   0    0    0 ...  169  991  218]\n",
            " [   0    0    0 ...  994 2182  177]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWhzkkaB5vcE",
        "outputId": "852ac3f0-9446-4583-d9e4-5131a282596a"
      },
      "source": [
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets,labels, test_size=0.20, random_state=42)\n",
        "\n",
        "print (len(X_train), len(X_test), len(y_train), len(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "688361 172091 688361 172091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoSjz1ar6cT1"
      },
      "source": [
        "# Model building\n",
        "\n",
        "Alright, in the next cells I'll guide you through the process of building 3 Recurrent Neural Networks. I'll implement sequential models from the Keras API to achieve this task. Essentially, I'll start with a single layer **LSTM** network which is known by achieving good results in NLP tasks when the dataset is relatively small (I could have started with a SimpleRNN which is even simpler, but to be honest it's actually not deployed in production environments because it is too simple - however I'll leave it commented in case you want to know it's built). The next one will be a Bidirectional LSTM model, a more complex one and this particular one is known to achieve great metrics when talking about text classification. To go beyond the classic NLP approach, finally we'll implement a very unusual model: a Convolutional 1D network, known as well by delivering good metrics when talking about NLP. If everything goes ok, we should get the best results with the BidRNN, let's see what happens.\n",
        "\n",
        "Let's get hands on:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3zv75fm63Ej"
      },
      "source": [
        "## LSTM layer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IkAvgJw5y9r",
        "outputId": "ca7886ed-0eda-475c-9523-b4453a01b38c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_words, 20))\n",
        "model.add(layers.LSTM(units=10, dropout=0.5))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Implementing model checkpoins to save the best metric and do not lose it on training.\n",
        "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', period=1, save_weights_only=False)\n",
        "\n",
        "batch_size=24 # 688361 samples / batch_size = Number of iterations per epoch\n",
        "history = model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test), callbacks=[checkpoint1], batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/15\n",
            "43023/43023 [==============================] - 475s 11ms/step - loss: 0.5423 - accuracy: 0.7805 - val_loss: 0.3916 - val_accuracy: 0.8577\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.85766, saving model to best_model1.hdf5\n",
            "Epoch 2/15\n",
            "43023/43023 [==============================] - 474s 11ms/step - loss: 0.4110 - accuracy: 0.8468 - val_loss: 0.3739 - val_accuracy: 0.8644\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.85766 to 0.86440, saving model to best_model1.hdf5\n",
            "Epoch 3/15\n",
            "43023/43023 [==============================] - 482s 11ms/step - loss: 0.3975 - accuracy: 0.8530 - val_loss: 0.3702 - val_accuracy: 0.8658\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.86440 to 0.86579, saving model to best_model1.hdf5\n",
            "Epoch 4/15\n",
            "43023/43023 [==============================] - 470s 11ms/step - loss: 0.3899 - accuracy: 0.8567 - val_loss: 0.3678 - val_accuracy: 0.8660\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.86579 to 0.86600, saving model to best_model1.hdf5\n",
            "Epoch 5/15\n",
            "43023/43023 [==============================] - 461s 11ms/step - loss: 0.3867 - accuracy: 0.8581 - val_loss: 0.3688 - val_accuracy: 0.8650\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.86600\n",
            "Epoch 6/15\n",
            "43023/43023 [==============================] - 466s 11ms/step - loss: 0.3834 - accuracy: 0.8596 - val_loss: 0.3651 - val_accuracy: 0.8671\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.86600 to 0.86712, saving model to best_model1.hdf5\n",
            "Epoch 7/15\n",
            "43023/43023 [==============================] - 463s 11ms/step - loss: 0.3824 - accuracy: 0.8599 - val_loss: 0.3638 - val_accuracy: 0.8679\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.86712 to 0.86790, saving model to best_model1.hdf5\n",
            "Epoch 8/15\n",
            "43023/43023 [==============================] - 470s 11ms/step - loss: 0.3819 - accuracy: 0.8600 - val_loss: 0.3651 - val_accuracy: 0.8673\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.86790\n",
            "Epoch 9/15\n",
            "43023/43023 [==============================] - 465s 11ms/step - loss: 0.3803 - accuracy: 0.8606 - val_loss: 0.3772 - val_accuracy: 0.8616\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.86790\n",
            "Epoch 10/15\n",
            "43023/43023 [==============================] - 469s 11ms/step - loss: 0.3788 - accuracy: 0.8606 - val_loss: 0.3963 - val_accuracy: 0.8556\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.86790\n",
            "Epoch 11/15\n",
            "43023/43023 [==============================] - 478s 11ms/step - loss: 0.3798 - accuracy: 0.8608 - val_loss: 0.4087 - val_accuracy: 0.8530\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.86790\n",
            "Epoch 12/15\n",
            "43023/43023 [==============================] - 471s 11ms/step - loss: 0.3810 - accuracy: 0.8601 - val_loss: 0.4119 - val_accuracy: 0.8536\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.86790\n",
            "Epoch 13/15\n",
            "43023/43023 [==============================] - 462s 11ms/step - loss: 0.3800 - accuracy: 0.8612 - val_loss: 0.4039 - val_accuracy: 0.8582\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.86790\n",
            "Epoch 14/15\n",
            "19542/43023 [============>.................] - ETA: 3:47 - loss: 0.3763 - accuracy: 0.8621"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7gnmBYmJgJp"
      },
      "source": [
        "# Best model validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUv7BQms69vn"
      },
      "source": [
        "# Load the best model obtained during training\n",
        "model = keras.models.load_model(\"best_model1.hdf5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print('Model accuracy: ',test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1NifRANJpmD"
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhrfjDB9JrO4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrjcqUFxLX_H"
      },
      "source": [
        "## Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMgFwrTiLjqu"
      },
      "source": [
        "sentiment = ['Negative', 'Neutral','Positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4cAyRIiKOt4"
      },
      "source": [
        "sequence = tokenizer.texts_to_sequences(['this experience has been the best, want my money back'])\n",
        "\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8IMa258Q31T"
      },
      "source": [
        "sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\n",
        "\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4pB34hsLaef"
      },
      "source": [
        "sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are fantastic'])\n",
        "\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvdpp7TELcxU"
      },
      "source": [
        "sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\n",
        "\n",
        "test = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iewDKowDBIKS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}