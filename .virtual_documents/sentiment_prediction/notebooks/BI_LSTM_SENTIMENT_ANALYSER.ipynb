import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import warnings
warnings.filterwarnings("ignore")


import re
import matplotlib.pyplot as plt
import string
import pandas as pd
import numpy as np
import csv

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from nltk.tokenize.treebank import TreebankWordDetokenizer

from gensim.utils import simple_preprocess

import tensorflow as tf
import keras

import os.path
from os import path


from tensorflow.python.client import device_lib

tf.config.list_physical_devices('GPU')
device_lib.list_local_devices()


dataset = pd.read_csv("../data/raw/datasetBalanced.csv") \
            .drop_duplicates(subset='text', keep="last")[['text', 'sentiment']]
# Remove duplicates & keep columns to use

dataset.head(5)


# Get the dataset lenght
len(dataset)


# Validate if there are different values from negative (0), neutral (1) and positive (2)
dataset['sentiment'].unique()


# How distributed is the dataset
dataset.groupby('sentiment').nunique()


# Fill null values.
dataset["text"].fillna("No content", inplace=True)


url_pattern = re.compile(r'https?://\S+|www\.\S+')
TAG_RE = re.compile(r'<[^>]+>')
EMOJI_RE = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)

def depure_data(data):
    # Removing URLs with a regular expression
    data = url_pattern.sub(r'', data)

    # Remove Emails
    data = re.sub('\S*@\S*\s?', '', data)

    # Remove new line characters
    data = re.sub('\s+', ' ', data)

    # Remove distracting single quotes
    data = re.sub("\'", "", data)

     #Remove @ sign
    data = re.sub("@[A-Za-z0-9]+","",data)

     #Remove http links
    data = re.sub(r"(?:@|http?://|https?://|www)\S+", "", data)

    #Remove Emojis
    data = EMOJI_RE.sub('', data)

    #Remove hashtag sign but keep the text
    data = re.sub("#[A-Za-z0-9]+","",data)

    # Remove html tags
    data = TAG_RE.sub('', data);

    return data


list_words = stopwords.words('english')

tweets = dataset['text'].values.tolist()
tweets_len = len(tweets)

remove_stopwords = lambda data: (" ").join([word for word in data.split() if not word in list_words]) # Remove unused words like will
processor = lambda sentence: simple_preprocess(str(sentence), deacc=True) # Lowecase, ponctuation & accents
detokenizer = lambda sequence: TreebankWordDetokenizer().detokenize(sequence) # Join sequence of tokens

data = list([detokenizer(processor(remove_stopwords(depure_data(tweets[i])))) for i in range(tweets_len)])

print(data[:5])


data = np.array(data)

# Save cleaned data for performance purposes
pd.DataFrame(data).to_csv('../data/pre_processing/pre_processed_tweets.csv')


labels = tf.keras.utils.to_categorical(dataset['sentiment'], 3, dtype="float32")
print(labels)


from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop,Adam
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras import regularizers
from keras import backend as K
from keras.callbacks import ModelCheckpoint

from sklearn.model_selection import train_test_split


max_words = 5000
max_len = 300


tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data)
sequences = tokenizer.texts_to_sequences(data)
tweets = pad_sequences(sequences, maxlen=max_len)
print(tweets)


# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(tweets,labels, test_size=0.20, random_state=42)

print (len(X_train), len(X_test), len(y_train), len(y_test))


model = Sequential()

model.add(layers.Embedding(max_words, 20))
model.add(layers.Bidirectional(layers.LSTM(10, dropout=0.5, return_sequences=True)))
model.add(layers.LSTM(units=10, dropout=0.5))
model.add(layers.Dense(3, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Implementing model checkpoins to save the best metric and do not lose it on training.
checkpoint1 = ModelCheckpoint("../models/0_bi_lstm_tweet_sentiment.hdf5", monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto', save_weights_only=False)

batch_size=100 # 688361 samples / batch_size = Number of iterations per epoch
history = model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test), callbacks=[checkpoint1], batch_size=batch_size)


# Load the best model obtained during training
model = keras.models.load_model("../models/0_bi_lstm_tweet_sentiment.hdf5")

test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)

print('Model accuracy: ',test_acc)


len(X_test)


predictions = model.predict(X_test)


import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()


sentiment = ['Negative', 'Neutral', 'Positive']


sequence = tokenizer.texts_to_sequences(['this experience has been the best, want my money back'])

test = pad_sequences(sequence, maxlen=max_len)

sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]


sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])

test = pad_sequences(sequence, maxlen=max_len)

sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]


sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are fantastic'])

test = pad_sequences(sequence, maxlen=max_len)

sentiment[np.around(model.predict(test), decimals=0).argmax(axis=1)[0]]


sequence = tokenizer.texts_to_sequences(['i really how the technician helped me with the issue that i had'])

test = pad_sequences(sequence, maxlen=max_len)

np.around(model.predict(test), decimals=0).argmax(axis=1)[0]


def prepare_prediction(text):
    clean = detokenizer(processor(remove_stopwords(depure_data(text))))

    sequence = tokenizer.texts_to_sequences([clean])

    text = pad_sequences(sequence, maxlen=max_len)

    return np.around(model.predict(text), decimals=0).argmax(axis=1)[0]


import glob
import time
from tqdm import tqdm

files = glob.glob('../../bitcoin_prediction/data/year_2021/month_6/*/data.csv', recursive=True)
filter_out = [
    'year_2022/month_2',
    'year_2022/month_1',
    'year_2021/month_12',
    'year_2021/month_11',
    'year_2021/month_10',
    'year_2021/month_9',
    'year_2021/month_8',
    'year_2021/month_7',
]

files = [x for x in files if
               all(y not in x for y in filter_out)]

print(len(files))
print("Estimated minutes = {}".format(len(files) * 37))
print(files)

for file in files:
    print("Starting file {}".format(file))
    file_dataset = pd.read_csv(file, lineterminator='\n')
    file_dataset["Sentiment"] = -1
    file_dataset_tweets = file_dataset['text'].values.tolist()

    for index in tqdm(range(len(file_dataset_tweets))):
        file_dataset["Sentiment"][index] = prepare_prediction(file_dataset_tweets[index])

    file_dataset.to_csv(file, index=False)
    print("Finnished file {}".format(file))
    



